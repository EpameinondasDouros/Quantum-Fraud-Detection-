{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO4E5pOLknkJScy8n7RBXJ5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from google.colab import files\n","from google.colab import drive\n","import sys\n","import os"],"metadata":{"id":"YWD9NFqA4L_I","executionInfo":{"status":"ok","timestamp":1730634910216,"user_tz":-120,"elapsed":9037,"user":{"displayName":"Epa Douros","userId":"06317381952326479974"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"7Hij2Dr-4IEF","executionInfo":{"status":"ok","timestamp":1730636382864,"user_tz":-120,"elapsed":275,"user":{"displayName":"Epa Douros","userId":"06317381952326479974"}}},"outputs":[],"source":["def load_data_from_kaggle(dataset_name, kaggle_file_path='/content/datasets', kaggle_json_drive_path=None):\n","    \"\"\"\n","    Downloads and loads data from Kaggle.\n","\n","    Parameters:\n","        dataset_name (str): Kaggle dataset identifier (e.g., 'ealaxi/paysim1').\n","        kaggle_file_path (str): Path where the dataset should be downloaded and unzipped.\n","        kaggle_json_drive_path (str): Optional Google Drive path for kaggle.json to avoid repeated uploads.\n","\n","    Returns:\n","        df (pd.DataFrame): Loaded DataFrame from the specified dataset.\n","    \"\"\"\n","    # Ensure kaggle.json is available\n","    if kaggle_json_drive_path:\n","        # Mount Google Drive if needed\n","        drive.mount('/content/drive')\n","        kaggle_json_path = os.path.join('/content/drive/MyDrive', kaggle_json_drive_path)\n","        !cp {kaggle_json_path} ~/.kaggle/kaggle.json\n","    else:\n","        # Upload kaggle.json manually\n","        files.upload()\n","\n","    # Set permissions and download dataset\n","    !chmod 600 ~/.kaggle/kaggle.json\n","    !kaggle datasets download -d {dataset_name} -p {kaggle_file_path} --unzip\n","\n","    print(f\"Dataset downloaded to: {kaggle_file_path}\")\n","\n","    # Load the dataset\n","    file_path = os.path.join(kaggle_file_path, \"PS_20174392719_1491204439457_log.csv\")\n","    df = pd.read_csv(file_path, delimiter=',', nrows=1000000)\n","\n","    return df\n","\n","def analyze_and_balance_data(df, target_column='isFraud', sampling_ratio=95/5, random_state=42):\n","    \"\"\"\n","    Analyzes the dataset and balances it according to the specified sampling ratio.\n","\n","    Parameters:\n","        df (pd.DataFrame): Original DataFrame.\n","        target_column (str): Target column for fraud classification.\n","        sampling_ratio (float): Ratio of non-fraud to fraud samples.\n","        random_state (int): Random seed for reproducibility.\n","\n","    Returns:\n","        balanced_df (pd.DataFrame): Balanced dataset with fraud and non-fraud transactions.\n","    \"\"\"\n","    print(f\"Original dataset shape: {df.shape}\")\n","\n","    fraud_df = df[df[target_column] == 1]\n","    non_fraud_df = df[df[target_column] == 0]\n","\n","    print(f\"Fraud dataset size: {fraud_df.shape}\")\n","    print(f\"Non-Fraud dataset size: {non_fraud_df.shape}\")\n","\n","    desired_non_fraud_count = int(len(fraud_df) * sampling_ratio)\n","    non_fraud_sampled = non_fraud_df.sample(n=desired_non_fraud_count, random_state=random_state)\n","\n","    balanced_df = pd.concat([fraud_df, non_fraud_sampled]).sample(frac=1, random_state=random_state).reset_index(drop=True)\n","    print(f\"Balanced dataset shape: {balanced_df.shape}\")\n","\n","    print(\"Class balance after sampling:\")\n","    print(balanced_df[target_column].value_counts(normalize=True) * 100)\n","\n","    return balanced_df\n","\n","def prepare_data_for_training(balanced_df, target_column='isFraud', top_features=None, test_size=0.2, random_state=42):\n","    \"\"\"\n","    Prepares data for training by selecting features, scaling, and splitting into train and test sets.\n","\n","    Parameters:\n","        balanced_df (pd.DataFrame): Balanced DataFrame with target and features.\n","        target_column (str): Target column for classification.\n","        top_features (list): List of top features to keep based on feature importance.\n","        test_size (float): Proportion of dataset to include in the test split.\n","        random_state (int): Random seed for reproducibility.\n","\n","    Returns:\n","        X_train, X_test, y_train, y_test: Train and test sets for model training.\n","    \"\"\"\n","    # One-hot encode categorical columns\n","    X = pd.get_dummies(balanced_df, columns=['type']).loc[:, ~balanced_df.columns.str.contains('name')]\n","    y = X[target_column]\n","    X = X.drop(columns=[target_column])\n","\n","    # Feature selection using RandomForest\n","    model = RandomForestClassifier(random_state=random_state)\n","    model.fit(X, y)\n","\n","    importances = model.feature_importances_\n","    feature_importance_df = pd.DataFrame({'feature': X.columns, 'importance': importances}).sort_values(by='importance', ascending=False)\n","\n","    print(\"Top 10 features based on importance:\")\n","    print(feature_importance_df.head(10))\n","\n","    if top_features is None:\n","        top_features = feature_importance_df['feature'].head(3).tolist()\n","\n","    X_top = X[top_features]\n","\n","    # Scale features\n","    scaler = StandardScaler()\n","    X_top = scaler.fit_transform(X_top)\n","\n","    # Split dataset\n","    X_train, X_test, y_train, y_test = train_test_split(X_top, y, test_size=test_size, random_state=random_state)\n","\n","    print(\"Training set size:\", X_train.shape, y_train.shape)\n","    print(\"Testing set size:\", X_test.shape, y_test.shape)\n","\n","    return X_train, X_test, y_train, y_test\n"]},{"cell_type":"code","source":["def prepare_data_for_training(balanced_df, target_column='isFraud', top_features=None, test_size=0.2, random_state=42):\n","    \"\"\"\n","    Prepares data for training by selecting features, scaling, and splitting into train and test sets.\n","\n","    Parameters:\n","        balanced_df (pd.DataFrame): Balanced DataFrame with target and features.\n","        target_column (str): Target column for classification.\n","        top_features (list): List of top features to keep based on feature importance.\n","        test_size (float): Proportion of dataset to include in the test split.\n","        random_state (int): Random seed for reproducibility.\n","\n","    Returns:\n","        X_train, X_test, y_train, y_test: Train and test sets for model training.\n","    \"\"\"\n","    # One-hot encode categorical columns\n","    X = pd.get_dummies(balanced_df, columns=['type']).loc[:, ~balanced_df.columns.str.contains('name')]\n","    y = X[target_column]\n","    X = X.drop(columns=[target_column])\n","\n","    # Feature selection using RandomForest\n","    model = RandomForestClassifier(random_state=random_state)\n","    model.fit(X, y)\n","\n","    importances = model.feature_importances_\n","    feature_importance_df = pd.DataFrame({'feature': X.columns, 'importance': importances}).sort_values(by='importance', ascending=False)\n","\n","    print(\"Top 10 features based on importance:\")\n","    print(feature_importance_df.head(10))\n","\n","    if top_features is None:\n","        top_features = feature_importance_df['feature'].head(3).tolist()\n","\n","    X_top = X[top_features]\n","\n","    # Scale features\n","    scaler = StandardScaler()\n","    X_top = scaler.fit_transform(X_top)\n","\n","    # Split dataset\n","    X_train, X_test, y_train, y_test = train_test_split(X_top, y, test_size=test_size, random_state=random_state)\n","\n","    print(\"Training set size:\", X_train.shape, y_train.shape)\n","    print(\"Testing set size:\", X_test.shape, y_test.shape)\n","\n","    return X_train, X_test, y_train, y_test"],"metadata":{"id":"MuEqmKRA4JnH"},"execution_count":null,"outputs":[]}]}